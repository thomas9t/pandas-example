{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy and SciPy in Action\n",
    "\n",
    "In this section we'll use NumPy and SciPy to write a custom data analysis algorithm for the data we just cleaned. We'll use Friedman's \"Projection Pursuit Regression\" (PPR) algorithm which is sort of like a souped up neural network. There are no widely used existing implementations of PPR for Python so we can't take the easy way out and use SciKit learn.\n",
    "\n",
    "PPR is a generalization of a \"generalized additive model\" (GAM) which expresses a function as a linear combination of smooth functions of a set of predictors. PPR extends the GAM to allow nonlinear relationships between predictors. Intuitively it can be thought of as iteratively finding the \"directions\" in the data which explain the most variance.\n",
    "\n",
    "PPR approximates a function as:\n",
    "\n",
    "$$y = \\sum_{h=1}^{H}g_{h}(w_{h}^{T}x)$$\n",
    "\n",
    "where the \"g\" are smooth nonparameteric functions (typically a spline). PPR and it's more widely known relative, the multilayer perceptron, are part of a larger family of approximation techniques based on \"ridge functions.\" PPR has several advantages which make it generally preferable to an MLP:\n",
    "\n",
    "1. Generally requires far fewer hidden units to approximate a funtion to the same level of accuracy\n",
    "2. Simpler algorithm for estimation\n",
    "3. Much less hyperparameter tuning\n",
    "\n",
    "The model is estimated using a straightforward algorithm based on iteratively reweighted least squares. Let's just consider fitting a single ridge function first and assume that we're minimizing L2 norm. Let `X` be an `R^{N x K}` matrix of data and let `y` be an `N` vector of outcomes in `R`.\n",
    "\n",
    "1. Initialize a vector of weights `w = random(k,1)`\n",
    "2. Compute `v = Xw`\n",
    "3. Estimate the spline `g(v)` minimizing `||y - g(v)||`. We can either use a smoothing spline, or choose the knots by hand. I've found it's best to choose knots at quantiles of `v`.\n",
    "4. Update the vector of weights as the solution to a weighted least squares problem: `w = inv(t(X)WX)(t(X)Wm)`. `W` is an `N x N` diagonal matrix such that `diag(W)[i] = g'(v[i])^2` and `m` is an `N` vector: `m = v + ((y - g(v)) / (g'(v))` where `g'(v)` denotes the first derivative of `g` evaluated at `v`.\n",
    "\n",
    "Then to add more directions we simply keep applying the algorithm above until error stops decreasing. The only change is that instead of `y` we use the residuals after previous terms. Fun fact: this method will always yield the model with lowest residual variance in the limit!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as alg\n",
    "import scipy.interpolate as ipl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PPRNode:\n",
    "    \"\"\"\n",
    "    Simple container to hold the parameters associated with a PPR node\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weights=None, spline=None, mu=0):\n",
    "        self.weights = weights\n",
    "        self.spline = spline\n",
    "        self.mu = 0\n",
    "        \n",
    "class ProjectionPursuitRegressor:\n",
    "    def __init__(self, knots=2, max_terms=15, train_split=0.8,\n",
    "                 tolerance=1.0, inner_iter_tol=1e-5):\n",
    "        self._smoother = self._least_squares_spline\n",
    "        self._knots = knots\n",
    "        self._params = []\n",
    "        self._test_mse = []\n",
    "        self._train_mse = []\n",
    "        self._train_split = train_split\n",
    "        self._max_terms = max_terms\n",
    "        self._outer_iter_tol = tolerance\n",
    "        self._inner_iter_tol = inner_iter_tol\n",
    "        self._stabilizer = 3.2\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # A common \"gotcha\" with NumPy involves accidentally\n",
    "        # reshaping two arrays into a matrix through broadcasting.\n",
    "        # Unraveling vectors helps avoid this\n",
    "        y = y.ravel()\n",
    "        \n",
    "        # Let's split our input data into a training and validation set\n",
    "        split = self._train_test_split(X, y, self._train_split)\n",
    "        X_train, y_train, X_test, y_test = split\n",
    "\n",
    "        # now we actually run the optimization\n",
    "        num_terms = 0\n",
    "        mse = np.inf\n",
    "        residuals = y_train\n",
    "        while num_terms < self._max_terms:            \n",
    "            # This call adds a new term to the model\n",
    "            self._fit_term(X_train, residuals)\n",
    "            \n",
    "            # now recompute goodness of fit metrics\n",
    "            mse_new_train = self._compute_mse(X_train, y_train)\n",
    "            mse_new_test = self._compute_mse(X_test, y_test)\n",
    "            self._train_mse.append(mse_new_train)\n",
    "            self._test_mse.append(mse_new_test)\n",
    "            \n",
    "            # check to see if we're within the tolerance for convergence\n",
    "            if np.abs(mse_new_train - mse) < self._outer_iter_tol:\n",
    "                break\n",
    "            \n",
    "            # otherwise add a new term and continue\n",
    "            residuals = y_train - self.predict(X_train)\n",
    "            mse = mse_new_train\n",
    "            num_terms += 1\n",
    "            \n",
    "            msg = 'Term: {} fit. Train MSE: {} => Test MSE: {}'\n",
    "            print msg.format(num_terms, mse_new_train, mse_new_test)\n",
    "        \n",
    "        msg = 'Converged! Final Train MSE: {} => Test MSE: {}'\n",
    "        print msg.format(mse_new_train, mse_new_test)\n",
    "        # now pare back the model to the one which gave best test performance\n",
    "        self._params = self._params[:np.argmin(self._test_mse)+1]\n",
    "    \n",
    "    def _fit_term(self, X, y):\n",
    "        \"\"\"\n",
    "        Internal workhorse function to fit an individual term using\n",
    "        iteratively reweighted least squares.\n",
    "        \"\"\"\n",
    "\n",
    "        node = PPRNode()\n",
    "        self._params.append(node)\n",
    "        \n",
    "        # initialize the weights with uniform random values\n",
    "        w = np.random.rand(X.shape[1],1).ravel()\n",
    "        \n",
    "        converged = False\n",
    "        cost = np.Inf\n",
    "        iteration = 0\n",
    "        while not converged:\n",
    "            # solve the weight update equations\n",
    "            v = X.dot(w)\n",
    "            g, g_prime, spline = self._smoother(v, y)\n",
    "            eps = y - g                                \n",
    "            g_prime_squared = np.power(g_prime,2)   \n",
    "            m = v + np.divide(eps, g_prime)            \n",
    "            w = self._wls(X, g_prime_squared, m)\n",
    "\n",
    "            node.spline = spline                       \n",
    "            node.weights = w\n",
    "\n",
    "            # recompute goodness of fit and check if the term has converged\n",
    "            cost_new = self._compute_mse(X, y)\n",
    "            if np.abs(cost-cost_new) < self._inner_iter_tol:\n",
    "                converged = True\n",
    "            if iteration > 100:\n",
    "                break\n",
    "            cost = cost_new\n",
    "            iteration += 1\n",
    "            \n",
    "    def _wls(self, X, w, y):\n",
    "        \"\"\"\n",
    "        Internal method to fit a weighted least squares model\n",
    "        \"\"\"\n",
    "\n",
    "        XTW = X.T*w\n",
    "        return self._solve_linear_system(XTW.dot(X), XTW.dot(y))\n",
    "\n",
    "    def _solve_linear_system(self, A, b):\n",
    "        \"\"\"\n",
    "        Solves a system of equations Ax = b using ridge regularization\n",
    "        \"\"\"\n",
    "\n",
    "        AA = A.copy()\n",
    "        np.fill_diagonal(AA, self._stabilizer+np.diag(A))\n",
    "        return alg.solve(AA, b)\n",
    "    \n",
    "    def _least_squares_spline(self, v, y):\n",
    "        \"\"\"\n",
    "        Fits y = g(v) where g is a cubic spline with the stipulated knots. \n",
    "        \"\"\"\n",
    "        ixs = np.argsort(v)\n",
    "        _v = v[ixs]\n",
    "        _y = y[ixs]\n",
    "        \n",
    "        # sometimes the values end up too close together due to numerical\n",
    "        # imprecision. If this happens just jitter them a bit.\n",
    "        # it'll all shake out later.\n",
    "        if not np.all(np.diff(_v) > 0.0):\n",
    "            v = v + np.random.uniform(1e-9, 1e-8, v.shape)\n",
    "            ixs = np.argsort(v)\n",
    "            _v = v[ixs]\n",
    "            _y = y[ixs]\n",
    "            \n",
    "        knots_actual = np.percentile(_v, np.linspace(0, 100, self._knots))\n",
    "        if np.abs(knots_actual[0] - _v[0]) < 1e-6:\n",
    "            knots_actual = knots_actual[1:]\n",
    "        if np.abs(knots_actual[-1] - _v[-1]) < 1e-6:\n",
    "            knots_actual = knots_actual[:-1]        \n",
    "        spline = ipl.LSQUnivariateSpline(_v, _y, knots_actual, k=3)\n",
    "        deriv = spline.derivative()\n",
    "        return (spline(v), deriv(v), spline)\n",
    "            \n",
    "    def _compute_mse(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute the goodness of fit for this model (as measured by MSE)\n",
    "        \"\"\"\n",
    "\n",
    "        y_hat = self.predict(X)\n",
    "        return np.mean(np.power(y - y_hat, 2))\n",
    "    \n",
    "    def predict(self, X, leave_out=None):\n",
    "        \"\"\"\n",
    "        Internal workhorse function to form predicted values for the current\n",
    "        state of the model\n",
    "        \"\"\"\n",
    "\n",
    "        directions = np.array([p.spline(X.dot(p.weights)) for p in self._params])\n",
    "        y = np.zeros(directions[0].shape)\n",
    "        for direction in directions:\n",
    "            y = y + direction\n",
    "        return y\n",
    "        \n",
    "    @staticmethod\n",
    "    def _train_test_split(X, y, split_pct):\n",
    "        is_test_set = (np.random.rand(X.shape[0],1) > split_pct).ravel()\n",
    "        X_test = X[is_test_set,:]\n",
    "        y_test = y[is_test_set]\n",
    "        X_train = X[np.logical_not(is_test_set),:]\n",
    "        y_train = y[np.logical_not(is_test_set)]\n",
    "        return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's see our model in action on some synthetic and real data.\n",
    "\n",
    "We'll show that our model is capable of learning an interaction term and then look at it for our real data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: 1 fit. Train MSE: 36811.0915102 => Test MSE: 38012.5584023\n",
      "Term: 2 fit. Train MSE: 3270.76293224 => Test MSE: 3415.73706242\n",
      "Term: 3 fit. Train MSE: 519.753696355 => Test MSE: 569.185960829\n",
      "Term: 4 fit. Train MSE: 266.624001123 => Test MSE: 272.376870423\n",
      "Term: 5 fit. Train MSE: 44.1814674237 => Test MSE: 46.1097368758\n",
      "Term: 6 fit. Train MSE: 15.7774777042 => Test MSE: 14.6214671089\n",
      "Term: 7 fit. Train MSE: 10.602041946 => Test MSE: 11.3277039963\n",
      "Term: 8 fit. Train MSE: 8.55168639181 => Test MSE: 8.08262752046\n",
      "Term: 9 fit. Train MSE: 6.68101197363 => Test MSE: 7.51201681698\n",
      "Term: 10 fit. Train MSE: 3.97409418426 => Test MSE: 5.23783807172\n",
      "Converged! Final Train MSE: 3.0406406737 => Test MSE: 3.82161850765\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(19832)\n",
    "\n",
    "# We'll generate a synthetic function with a nonlinear interaction\n",
    "# between two variables and then jitter it with gaussian white noise\n",
    "X = np.random.rand(1000,10)*100\n",
    "XR = np.concatenate((np.ones((1000,1)), X.copy()), axis=1)\n",
    "X = np.concatenate((np.ones((1000,1)), X, (X[:,1]*X[:,2]).reshape(1000,1),\n",
    "                   (X[:,1]*X[:,3]).reshape(1000,1)), axis=1)\n",
    "y = X.dot(np.random.rand(X.shape[1],1)) + np.random.normal(0,1,(1000,1))\n",
    "\n",
    "# we should rescale our data to have zero mean and unit variance\n",
    "# note how broadcasting makes this really convenient\n",
    "X = (X[:,1:] - X[:,1:].mean(axis=0)) / np.sqrt(X[:,1:].var(axis=0))\n",
    "XR = (XR[:,1:] - XR[:,1:].mean(axis=0)) / np.sqrt(XR[:,1:].var(axis=0))\n",
    "\n",
    "P = ProjectionPursuitRegressor()\n",
    "P.fit(XR, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     NMHC(GT)  C6H6(GT)   NOx(GT)   NO2(GT)         T  \\\n",
      "timestamp                                                               \n",
      "2004-03-10 21:00:00  0.826228  0.263163  0.626631  0.797512  0.089890   \n",
      "2004-03-10 22:00:00  1.532394  0.117622 -0.154504  0.467118  0.036331   \n",
      "2004-03-10 23:00:00  1.437413  0.072096 -0.321908  0.304744  0.036331   \n",
      "2004-03-11 00:00:00  1.386269  0.044274 -0.429525  0.150489  0.038751   \n",
      "2004-03-11 01:00:00  1.386269  0.036686 -0.429525  0.142371  0.024231   \n",
      "\n",
      "                           RH        AH  NMHC(GT)_L1  C6H6(GT)_L1  NOx(GT)_L1  \\\n",
      "timestamp                                                                       \n",
      "2004-03-10 21:00:00  0.246092  0.207562     1.802197     0.180861   -0.154448   \n",
      "2004-03-10 22:00:00  0.411657  0.206929     0.825863     0.263172    0.626692   \n",
      "2004-03-10 23:00:00  0.403492  0.206822     1.531911     0.117630   -0.154448   \n",
      "2004-03-11 00:00:00  0.354502  0.206163     1.436945     0.072104   -0.321853   \n",
      "2004-03-11 01:00:00  0.419823  0.206429     1.385810     0.044282   -0.429471   \n",
      "\n",
      "                        ...      4  5  6  7  8  9  10  11  12  is_morning  \n",
      "timestamp               ...                                                \n",
      "2004-03-10 21:00:00     ...      0  0  0  0  0  0   0   0   0           0  \n",
      "2004-03-10 22:00:00     ...      0  0  0  0  0  0   0   0   0           0  \n",
      "2004-03-10 23:00:00     ...      0  0  0  0  0  0   0   0   0           0  \n",
      "2004-03-11 00:00:00     ...      0  0  0  0  0  0   0   0   0           1  \n",
      "2004-03-11 01:00:00     ...      0  0  0  0  0  0   0   0   0           1  \n",
      "\n",
      "[5 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "# Now let's give it a try on our real dataset\n",
    "air = pd.read_csv('clean_air_data.csv')\n",
    "air = air.set_index(pd.to_datetime(air['timestamp'])).sort_index()\n",
    "\n",
    "cols_to_drop = filter(lambda x: 'PT08' in x, air.columns)\n",
    "air = air.drop(cols_to_drop, axis='columns')\n",
    "\n",
    "# our data is a time-series so we'll model it using a finite distributed lag\n",
    "# with three periods\n",
    "# and include fixed-effects for month of year and AM vs. PM\n",
    "\n",
    "air['month'] = air.index.map(lambda x: x.month)\n",
    "air['is_morning'] = air.index.map(lambda x: x.hour < 12)\n",
    "fe_cols = pd.get_dummies(\n",
    "        air['month'], drop_first=True\n",
    "    ).join(air['is_morning'].astype(np.int32), how='inner')\n",
    "\n",
    "y = air['CO(GT)'].values.ravel()[3:]\n",
    "depvars = air.drop(['timestamp','CO(GT)','month','is_morning'], axis='columns').sort_index()\n",
    "X = depvars.join(\n",
    "        depvars.shift(1), how='inner', rsuffix='_L1'\n",
    "    ).join(\n",
    "        depvars.shift(2), how='inner', rsuffix='_L2'\n",
    "    ).join(\n",
    "        depvars.shift(3), how='inner', rsuffix='_L3'\n",
    "    ).dropna()\n",
    "\n",
    "# now let's also rescale this to zero mean\n",
    "X = (X - X.mean(axis=0)) / np.sqrt(X.var(axis=0))\n",
    "\n",
    "# and lastly join on the fixed effects\n",
    "X = X.join(fe_cols, how='inner')\n",
    "print X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (OLS): 2445.27952118\n"
     ]
    }
   ],
   "source": [
    "# Now let's compare PPR and a vanilla least squares reression\n",
    "# Both of these are actually terrible models for this dataset\n",
    "# but it's illustrative of how to use NumPy so let's just go with it...\n",
    "\n",
    "X = X.values\n",
    "\n",
    "# Let's also partition our data into a train and test set\n",
    "is_test = np.random.rand(X.shape[0],1).ravel() > 0.80\n",
    "X_test = X[is_test,:]\n",
    "y_test = y[is_test]\n",
    "X_train = X[np.logical_not(is_test),:]\n",
    "y_train = y[np.logical_not(is_test)]\n",
    "\n",
    "b = alg.solve(X_train.T.dot(X_train), X_train.T.dot(y_train))\n",
    "y_hat = X_test.dot(b)\n",
    "eps = y_test - y_hat\n",
    "print 'MSE (OLS): {}'.format(np.power(eps,2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: 1 fit. Train MSE: 2034.55162507 => Test MSE: 2226.96726137\n",
      "Term: 2 fit. Train MSE: 1912.92038718 => Test MSE: 2135.45174834\n",
      "Term: 3 fit. Train MSE: 1828.67961698 => Test MSE: 2036.7297136\n",
      "Term: 4 fit. Train MSE: 1769.67349769 => Test MSE: 2060.2256412\n",
      "Term: 5 fit. Train MSE: 1734.68225858 => Test MSE: 2016.76803863\n",
      "Term: 6 fit. Train MSE: 1714.23969093 => Test MSE: 2009.55890516\n",
      "Term: 7 fit. Train MSE: 1690.48480444 => Test MSE: 2013.08279575\n",
      "Term: 8 fit. Train MSE: 1672.91337456 => Test MSE: 2015.56052184\n",
      "Term: 9 fit. Train MSE: 1644.69128253 => Test MSE: 2003.18421334\n",
      "Term: 10 fit. Train MSE: 1628.83548853 => Test MSE: 1996.69945616\n",
      "Term: 11 fit. Train MSE: 1614.24884996 => Test MSE: 1987.56259109\n",
      "Term: 12 fit. Train MSE: 1591.28778643 => Test MSE: 1987.03894426\n",
      "Term: 13 fit. Train MSE: 1575.56993104 => Test MSE: 1986.09438947\n",
      "Term: 14 fit. Train MSE: 1534.23810555 => Test MSE: 1971.27269044\n",
      "Term: 15 fit. Train MSE: 1526.08589199 => Test MSE: 1963.47666131\n",
      "Converged! Final Train MSE: 1526.08589199 => Test MSE: 1963.47666131\n",
      "MSE (PPR): 1791.86217074\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(13298)\n",
    "P = ProjectionPursuitRegressor()\n",
    "P.fit(X_train, y_train)\n",
    "y_hat = P.predict(X_test)\n",
    "\n",
    "eps = y_test - y_hat\n",
    "print 'MSE (PPR): {}'.format(np.power(eps,2).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
